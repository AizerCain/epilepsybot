import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import re
import json

nltk.download('stopwords')
nltk.download('wordnet')


with open('epilepsy_faq.json') as f:
    data = json.load(f)

# Create dataset from intents
intent_data = []
for intent in data['intents']:
    for pattern in intent['patterns']:
        intent_data.append({
            'text': pattern,
            'tag': intent['tag']
        })

df = pd.DataFrame(intent_data)

# Map tags to risk levels
risk_mapping = {
    'symptoms': 'High Risk',
    'emergency': 'High Risk',
    'first_aid': 'Moderate Risk',
    'treatment': 'Moderate Risk',
    'medication': 'Moderate Risk',
    'triggers': 'Low Risk',
    'diet': 'Low Risk',
    'myths': 'Low Risk'
}

df['risk'] = df['tag'].map(risk_mapping)


lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))

def preprocess_text(text):
    text = text.lower()
    text = re.sub(r'[^\w\s]', '', text)
    tokens = text.split()
    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]
    return ' '.join(tokens)

df['processed'] = df['text'].apply(preprocess_text)


tokenizer = Tokenizer(num_words=1000, oov_token='<OOV>')
tokenizer.fit_on_texts(df['processed'])
sequences = tokenizer.texts_to_sequences(df['processed'])
padded = pad_sequences(sequences, maxlen=20)


from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
labels = le.fit_transform(df['risk'])
labels = tf.keras.utils.to_categorical(labels)

# Save label encoder
import joblib
joblib.dump(le, 'label_encoder.pkl')


model = tf.keras.Sequential([
    tf.keras.layers.Embedding(
        input_dim=1000,
        output_dim=64,
        input_length=20,
        mask_zero=True  # Improved sequence handling
    ),
    tf.keras.layers.Bidirectional(
        tf.keras.layers.LSTM(32, 
                           return_sequences=False,
                           dropout=0.3,  # Input dropout
                           recurrent_dropout=0.3)  # Recurrent dropout
    ),
    tf.keras.layers.Dropout(0.3),  # Increased from 0.5
    tf.keras.layers.Dense(24, activation='relu',
                         kernel_regularizer=tf.keras.regularizers.l1_l2(l1=0.01, l2=0.01)),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(3, activation='softmax')
])

model.compile(
    loss='categorical_crossentropy',
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0008),  # Reduced learning rate
    metrics=['accuracy']
)


early_stopping = tf.keras.callbacks.EarlyStopping(
    monitor='val_loss',
    patience=8,  # Increased patience for LSTM
    restore_best_weights=True,
    verbose=1
)

model_checkpoint = tf.keras.callbacks.ModelCheckpoint(
    'epilepsy_risk_model.h5',
    monitor='val_accuracy',
    save_best_only=True,
    mode='max',
    verbose=1
)

# Add learning rate scheduler
lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.5,
    patience=3,
    min_lr=1e-6,
    verbose=1
)

# Training with gradient clipping
history = model.fit(
    padded,
    labels,
    epochs=100,  # Increased max epochs
    batch_size=32,
    validation_split=0.2,
    class_weight=class_weight_dict,
    callbacks=[early_stopping, model_checkpoint, lr_scheduler],
    verbose=1,
    shuffle=True
)


# Save Keras model
model.save('epilepsy_risk_model.h5')

# Save tokenizer and preprocessing components
joblib.dump(tokenizer, 'tokenizer.pkl')
joblib.dump({
    'lemmatizer': lemmatizer,
    'stop_words': stop_words,
    'preprocess_text': preprocess_text
}, 'preprocessing.pkl')